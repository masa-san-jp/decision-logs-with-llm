# ローカルLLM統合型 Multi-Tool RAGシステム 設計仕様書

## 1. システム概要

本システムは、完全ローカル環境で稼働する大規模言語モデル（想定モデル：`gpt-oss-20b`）を中核推論エンジンとし、外部の最新Web情報と内部の固有ドキュメント（Markdown形式等）を統合して高精度な回答を生成するAIエージェントシステムである。
外部APIは純粋な「データ取得（フェッチ）」のみに利用し、取得した情報の「統合」「競合解決」「最終推論」はすべてローカルLLM内で完結させることで、機密情報の保護と柔軟な文脈構築を両立する。

## 2. システムアーキテクチャ

本システムは、以下の主要コンポーネントで構成される。

| コンポーネント | 役割・技術スタック |
| --- | --- |
| **オーケストレーター** | ユーザー入力の受付、各モジュールの非同期呼び出し制御、プロンプト構築（Python, `asyncio`） |
| **外部検索モジュール** | 複数のWeb検索API（例：Tavily API, Brave Search API等）への並列リクエストとテキストスニペットの抽出 |
| **内部検索モジュール** | ローカルのMarkdownファイルを格納したベクトルデータベース（例：ChromaDB, FAISS）と埋め込みモデルによる類似度検索 |
| **推論エンジン** | ローカルLLM推論サーバー（Ollama, vLLM等）上で稼働する `gpt-oss-20b` |

## 3. 処理フロー

システムは以下の3フェーズで順次実行される。

### フェーズ1: 情報の並列収集（Parallel Fetching）

ユーザーからのクエリ入力後、オーケストレーターは非同期処理を用いて複数ソースから同時に情報を収集する。

1. **Web検索（外部）:** 複数の検索APIへクエリを送信。生のHTMLではなく、要約された「テキストスニペット」のみを取得し、重複URLを排除する。
2. **ローカルRAG（内部）:** ベクトルDBを用いて、クエリと意味的に類似する内部Markdownファイルのチャンク（段落群）を抽出する。

### フェーズ2: 情報統合と競合解決（Context Synthesis）

収集したテキストデータを統合プロンプトに組み込み、ローカルLLM（`gpt-oss-20b`）へ1回目の推論リクエストを送信する。

* **目的:** 情報の圧縮、ノイズ除去、および情報源同士の矛盾解決を行い、中間コンテキスト（統合レポート）を生成する。
* **競合解決ロジック:** Web情報と内部情報間で内容の矛盾が生じた場合、**「内部情報」をグラウンドトゥルース（絶対的正解）として優先**して採用するようプロンプトで強制する。

### フェーズ3: 最終回答生成（Final Generation）

フェーズ2で出力された「精製済み中間コンテキスト」をシステムプロンプトとして設定し、ローカルLLM（`gpt-oss-20b`）へ2回目の推論リクエストを送信する。
この推論結果を、ユーザーへの最終回答として出力（ストリーミング出力）する。

## 4. プロンプト設計仕様

20Bクラスのモデルにおける「Lost in the Middle現象（長文コンテキストの中間部分の注意力が低下する現象）」を回避するため、優先度の高い情報をプロンプトの末尾に配置する。

### フェーズ2（統合用）プロンプト構造

```text
[システムプロンプト]
あなたは客観的な情報整理アシスタントです。以下の情報を統合し、ユーザーの質問「{user_query}」に答えるための要約レポートを作成してください。
ユーザーへの直接的な語りかけや挨拶は不要です。事実のみを箇条書きで抽出してください。
【重要ルール】Web情報と内部情報に矛盾がある場合は、必ず「内部情報」を正として優先して記載してください。

[入力データ]
▼ 外部Web情報
{external_web_snippets}

▼ 内部ドキュメント情報（最優先事項）
{internal_markdown_chunks}

```

### フェーズ3（最終生成用）プロンプト構造

```text
[システムプロンプト]
あなたは社内の専門アシスタントです。以下の【統合コンテキスト】のみに基づいて、ユーザーの質問に回答してください。コンテキストに存在しない情報を推測で補完しないでください。

【統合コンテキスト】
{synthesized_report_from_phase2}

[ユーザーの質問]
{user_query}

```

## 5. 非機能要件・制約事項

1. **コンテキスト長（トークン数）の制限と管理**
* フェーズ1で収集する総テキスト量は、`gpt-oss-20b` の有効コンテキストウィンドウ（想定: 4096〜8192トークン）の80%以内に収まるよう、オーケストレーター側で文字数によるTrimming（切り詰め）を行う。


2. **処理遅延（レイテンシ）への対策**
* 外部APIの呼び出しには厳格なタイムアウト（例：3〜5秒）を設定し、応答のないAPIはスキップして処理を続行する。
* フェーズ2およびフェーズ3の実行中は、ユーザーインターフェース上に進行状況（「情報を収集中…」「統合レポートを作成中…」等）を明示し、UXの低下を防ぐ。


3. **データフォーマットの最適化**
* 内部のMarkdownファイルには、検索精度向上のため、可能な限りフロントマター（YAML等）でメタデータ（作成日、対象カテゴリ等）を付与してベクトル化を行う。
